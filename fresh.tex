\documentclass[12pt]{article}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\linespread{1.5}

\begin{document}

\section{Introduction}

Perturbation theory has its roots in celestial mechanics and aerodynamics. There
are two particularly interesting victories the field has seen. The first was the
discovery of Neptune, and the second is the theoretical foundation for
aerodynamics. We will give a short account of these in the history section.

The promise of perturbation theory is that it allows us to solve a
larger class of problems than we could otherwise with analytical methods. It
does this by giving us approximate, but rigorous, solutions. In this article we
will guide the reader from some a very simple and familiar example, a quadratic
equation, all the way to a useful research example in system biology: the total
quasi steady state in enzyme kinetics.

The article has been written with the goal that any student with a basic
understanding of calculus, differential equations and linear algebra will be
able to follow along. All the relevant biology and perturbation theory will be
explained as we go along.

TODO: Write a subsection on the history of perturbation theory.

\section{A regular perturbation}

We are going to start with probably the simplest non-trivial example imaginable:
a quadratic equation.

\begin{equation}
x^2 - 2x + \epsilon = 0
\end{equation}

We know how to solve this analytically. The roots of the equation are $x = 1 +
\sqrt{1 - \epsilon}$ and $x = 1 - \sqrt{1 - \epsilon}$.

The case we are interested in is the one where $\epsilon$ is very small. In this
case, we can see that setting $\epsilon=0$ produces the roots $x=0$ and $x=2$
respectively. In fact, for any given small $\epsilon$ we notice that solution
changes very little. In that sense it's a very "boring" and predictable problem.

How can we make this observation more rigorous? One way is to rewrite the part
of the solution containing $\epsilon$ as a Taylor series. Recall that a Taylor
series for a function $f(x)$ around a looks like follows.

\begin{equation}
f(x) = \sum_{n=0}^{\infty} \frac{f^{n}(a)}{n!} (x-a)^n
\end{equation}

The Taylor series for $f(\epsilon) = (1 - \epsilon)^{1/2}$ around 0 is thus.

\begin{equation}
\sqrt{1 - \epsilon} = 1 - \frac{\epsilon}{2} - \frac{\epsilon^2}{8} + O(\epsilon^3)
\end{equation}

If we take the limit of this as $\epsilon \to 0$, it's obvious that our
intuition is correct. That is, a small change in epsilon only brings about a
small change in the solution.

\begin{align*}
x_1 &= \frac{\epsilon}{2} + \frac{\epsilon^2}{8} + O(\epsilon^3), \\
x_2 &= 2 - \frac{\epsilon}{2} - \frac{\epsilon^2}{8} + O(\epsilon^3)
\end{align*}

What is the point of all this? We said in the beginning that perturbation theory
allows us to solve a larger class of problems, but what we have done so far
hasn't given any indication of that being true. After all, we already have an
analytical formula for the quadratic equation.

Let's start over, but this time let's assume we don't have the quadratic formula
at our disposal. We will outline a method which would allow us to get arbitrarly
good approximations for polynomials of any degree.

Let's assume the solution of (1) can be expressed on the form of a power series
of $\epsilon$.

\begin{equation}
\sum_0^{\infty} a_k \epsilon^k = a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...
\end{equation}

We will now insert this power series into (1), and then expand that expression
in terms of powers of $\epsilon$. We only have to do this for the first few
terms of the power series, as we will see in the end. If we want to we can
always add more terms and get a more accurate approximation.

\begin{equation}
(a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...)^2 - 2(a_0 + a_1 \epsilon + a_2
\epsilon^2 + ...) + \epsilon = 0
\end{equation}

We expand the expression using Big-Oh algebra. For example, $(a_0 + a_1 \epsilon
+ a_2 \epsilon^2)^2 = a_0^2 + 2 a_0 a_1 \epsilon + (a_1^2 + 2 a_0 a_2) \epsilon^2 + O(\epsilon^3)$.

The whole expression becomes the following.

\begin{equation}
a_0^2 - 2 a_0 + (2 a_0 a_1 - 2 a_1 + 1)\epsilon + (a_1^2 + 2 a_0 a_2 - 2 a_2)
\epsilon^2 = O(\epsilon^3), \epsilon \to 0
\end{equation}

Now we turn the problem of determining the coefficients $a_0$, $a_1$ etc. Since
$\epsilon$ is a variable here rather than a parameter, we see that the
coefficients before each power of $\epsilon$ separately all have to be equal to
zero. For example, $a_0^2 - 2 a_0 = 0$ when $\epsilon \to 0$, and if we remove
that part and divide the rest by $\epsilon$ we get the same for the next
coefficient, etc. We thus get the following system of equations for solving the
coefficients.

\begin{align*}
a_0^2 - 2 a_0 &=0, \\
2 a_0 a_1 - 2 a_1 + 1 &= 0, \\
a_1^2 + 2 a_0 a_2 - 2 a_2 &= 0
\end{align*}

Solving these equations in turn gives us $a_0 = 0$ or $a_0 = 2$. For $a_0 = 0$ we
have $a_1 = \frac{1}{2}$ and $a_2 = \frac{1}{8}$. For $a_0 = 2$ we have $a_1 = -
\frac{1}{2}$ and $a_2 = - \frac{1}{8}$.

We said at the beginning that we assume the solution is on the form of a power
series of $\epsilon$. We have two options for the coefficients, and these
corresponds to the two approximate solutions.

\begin{align*}
x_1 &= \frac{1}{2} \epsilon + \frac{1}{8} \epsilon^2 + O(\epsilon^3), \\
x_2 &= 2 - \frac{1}{2} \epsilon - \frac{1}{8} \epsilon^2 + O(\epsilon^3)
\end{align*}

This corresponds well to our previous solution, without the use of the
quadratica formula. We have thus found a general method for finding approximate
solutions to polynomials of any degree with a small parameter $\epsilon$.

\section{A singular perturbation}

In the last section we dealt with a so called regular problem. In this section
we will deal with a singular perturbation problem. What is the difference? In a
singular perturbation problem, the small $\epsilon$ \textit{matters} for the
solution. In general, singular problems are generally interesting precisely
because their solutions can change a lot with just a small change in
circumstances.

As before, we will use a quadratic equation to illustrate how it works.

\begin{equation}
\epsilon x^2 - 2 x + 1 = 0
\end{equation}

As before, we take $\epsilon$ to be a very small number. This equation has the
following solutions.

\begin{align*}
x_1 &= \frac{1 + \sqrt{1 - \epsilon}}{\epsilon}, \\
x_2 &= \frac{1 - \sqrt{1 - \epsilon}}{\epsilon}.
\end{align*}

The first thing we notice is that even though $\epsilon$ is very small, we can't
set it to zero. If we were to do it in (8), we would only get a one degree
polynomial. The fact that we are losing solutions is a qualitative change, and
is indicative that we are dealing with a singular perturbation problem.

TODO: Skip this Taylor section, it's superfluous.

We can re-use the Taylor expansion of $\sqrt{1 - \epsilon}$ from (3), and we
would get that the solutions of our equation are.

TODO: Provide Taylor expansion.

\begin{equation}
x_1 = \frac{1 + \sqrt{1 - \epsilon}}{\epsilon}, \\
x_2 = \frac{1 - \sqrt{1 - \epsilon}}{\epsilon}.
\end{equation}

We have a problem here. As $\epsilon \to 0$, the root $x_2 \to \infty$. Is this
correct? Let's use the same method a we did before. We assume the solution can
be expressed in the form of a power series of $\epsilon$. Inserting this in our
equation gives us the following.

\begin{equation}
\epsilon (a_0 + a_1 \epsilon + a_2 \epsilon^2)^2 - 2(a_0 + \epsilon a_1 +
a_2 \epsilon^2 + ...) + 1 = 0
\end{equation}

which gets expanded into.

\begin{equation}
(- 2 a_0 + 1) + (a_0^2 - 2 a_1) \epsilon + (2 a_0 a_1 -2 a_2) \epsilon^2 +
O(\epsilon^3)
\end{equation}

and leads to.

\begin{align*}
- 2 a_0 + 1 &= 0, \\
a_0^2 - 2 a_1 &= 0, \\
2 a_0 a_1 -2 a_2 &= 0
\end{align*}

$a_0 = \frac{1}{2}$, $a_1=\frac{1}{8}$, $a_2= \frac{1}{64}$, but this gives us
only one of the roots, $x_1$.

\begin{equation}
x_1 = \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2)
\end{equation}

By the Fundamental Theorem of Algebra, we would expect to see two solutions.
What happened with the other root? We missed it because it's not on the form of
a perturbation series. We got a hint of this when we saw the Taylor expansion of
the root, but remember that we aren't supposed to use that. So what do we do?

The key here is that we can do a change in variable to turn the problem into a
regular perturbation problem.

\begin{equation}
x(\epsilon) = \frac{y(\epsilon)}{\delta(\epsilon)}
\end{equation}

Here we are treating $x$ as a function, $y(\epsilon)$ is $O(1)$ and we want to
determine the re-scaling factor function $\delta(\epsilon)$. Our original
equation becomes

\begin{equation}
\frac{\epsilon}{\delta^2} y^2 - \frac{2}{\delta} y + 1 = 0
\end{equation}

Our goal is to simplify the equation. We do this by dropping relatively
insignifcant terms. As we have seen, it turns out that the first term in the
equation is not insignificant, so we have to leave it in. Is there some other
term that we, to a first approximation, can drop?

All the three terms in the above equation have an order of magnitude. The method
of dominant balance tells us to look for pairs that balance, where balance means
they are of the same order of magnitude. We have already determined that the
first term can't be drop, so we have two options:

\textbf{Case 1.} $\frac{\epsilon}{\delta^2} y^2$ balances 1, with $\frac{2}{\delta}$
being relatively insignificant.

$\frac{\epsilon}{\delta^2} = 1$ implies $\delta = \epsilon^{\frac{1}{2}}$. But
then $\frac{2}{\delta} = \frac{2}{\epsilon^{\frac{1}{2}}}$ which isn't small as
$\epsilon \to 0$.

\textbf{Case 2.} $\frac{\epsilon}{\delta^2} y^2$ balance $\frac{2}{\delta} y$,
with 1 being relatively insignificant.

This means $\frac{\epsilon}{\delta^2} = \frac{1}{\delta}$ which implies that
$\delta = \epsilon$. This seems correct as both expressions are
$O(\frac{1}{e})$, and 1 is relatively small compared to that. We have

\begin{align}
P(x) = \epsilon x^2 - 2x + 1, \\
\epsilon P(\frac{y}{\epsilon}) = y^2 - 2 y + \epsilon
\end{align}

Where the last part is exactly the same as our regular perturbation problem.
Like before, this gives us

\begin{align}
y_1 &= \frac{1}{2} \epsilon + \frac{1}{8} \epsilon^2 + O(\epsilon^3), \\
y_2 &= 2 - \frac{1}{2} \epsilon - \frac{1}{8} \epsilon^2 + O(\epsilon^3)
\end{align}

which means that

\begin{align}
x_1 &= \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2), \\
x_2 &= \frac{2}{\epsilon} - \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2)
\end{align}

The second root is our missing solution, and it corresponds to $x \to \infty$ as
$\epsilon \to 0$. This is the essence of singular perturbation theory - to find
the singular behavior and do a change of variable to turn it into a regular
perturbation problem.

\section{ODE and boundary theory}

TODO: Section on basics of boundary theory, intuition?

\subsection{Outer solution}

We are now going to look at a differential equation.

\begin{equation}
\epsilon y'' + 2 y' + y, y(0)=0, y(1)=1, 0 < x < 1
\end{equation}

If we naively set $\epsilon = 0$ we see that the resulting equation $2 y' + y$
has the general solution $C e^{- \frac{1}{2}x}$. This can't satisfy both
boundary conditions at once. If it satisfies $y(0)=0$ we have $y=0$ as the only
solution, and if it satisfies $y(1)=1$ we have

\begin{equation}
y=e^{\frac{1}{2} (1 - x)}
\end{equation}

TODO: Why is this assumption legit?

We are going to assume this is a good approximation somewhere. We are also going
to assume this is valid for the $y(1)=1$ boundary condition.

Like the example in the last section, we missed something when we set $\epsilon
= 0$. We have found one of the two solutions and now we want to the find the
other one with the help of pairwise balancing.

\subsection{Inner solution}

We are assuming the boundary layer is near 0, and that it has a thickness
$\delta(\epsilon)$. We introduce a re-scaling variable

\begin{equation}
\overline{x} = \frac{x}{\delta}
\end{equation}

Having two scales like this is typical for singular perturbation problems. Our
original equation REF becomes

\begin{equation}
\frac{\epsilon}{\delta^2} \frac{d^2y}{d\overline{x}^2} + \frac{2}{\delta}
\frac{dy}{d\overline{x}} + y = 0
\end{equation}

We now do pairwise balancing, like before. We have the following orders of
magnitude

\begin{equation}
\frac{\epsilon}{\delta^2}, \frac{1}{\delta}, 1
\end{equation}

We must have $\frac{\epsilon}{\delta^2}$ present, so the question is if it
balance one of the other terms with one being insignificant, and if so, which one
is insignificant. There are two cases.

\textbf{Case 1}

\begin{equation}
\frac{\epsilon}{\delta^2} ~ 1 => \delta = \epsilon^{\frac{1}{2}},
\end{equation}

but then $\frac{1}{\delta}$ is big compared to 1.

textbf{Case 2}
\begin{equation}
\frac{\epsilon}{\delta^2} ~ \frac{1}{\delta} => \delta = \epsilon,
\end{equation}

in which case 1 is indeed insignificant.

We have, multiplying by $\epsilon$, the inner equation:

\begin{equation}
\frac{\delta^2 y}{\delta \overline{x}^2} + 2 \frac{dy}{d\overline{x}} + \epsilon
y = 0
\end{equation}

To a first approximation we can neglect the last term

\begin{equation}
\frac{\delta^2 y}{\delta \overline{x}^2} + 2 \frac{dy}{d\overline{x}} = 0
\end{equation}

Together with the other initial condition $y(0)=0$ we get the general inner
solution, valid in a region of thickness $\epsilon$ close to $x=0$

\begin{equation}
y_I = C(1 - e^{-2\overline{x}})
\end{equation}

We now have an outer and inner solution, and we turn to matching these to
determine the constant $C$.

\subsection{Matching}

The idea behind matching is that there is some edge or region between the inner
and outer solution. A region where both are valid approximations.

If we imagine a particle tracing the x-axis rightward, as we exit the boundary
layer, i.e. as $\overline{x} \to \infty$, the value of $y_I$ should be equal to
the value of $y_O$ as $x \to 0$, that is

\begin{equation}
lim \overline{x} \to |infty y_I = lim \to 0+ y_O \iff
lim \overline{x} \to |infty C(1 - e^{-2\overline{x}}) = lim \to 0+
e^{\frac{1}{2}(1-x)}
\end{equation}

This is a matching conditioning. The right-hand side is $e^{\frac{1}{2}}$, and
thus the left-hand side gives us that $C = e^{\frac{1}{2}}$ is required.

\subsection{Uniform approximation}

We now have two separate approximations. We would like to have one single
approximation that is valid everywhere. We do this by adding the two
approximations together and removing their common part.

\begin{equation}
y_U = y_o(x) + y_I(\frac{x}{\delta}) - e^{\frac{1}{2}}
\end{equation}

The common part comes from the matching condition in the previous section. In
the inner region the other terms are negliable, and vice versa in the outer
region.

\begin{align}
y_U &= e^{\frac{1}{2}(1-x)} e^{\frac{1}{2}}(1 - e^{\frac{-2x}{\epsilon}}) -
e^{\frac{1}{2}} \\
    &=  e^{\frac{1}{2}}(e^{\frac{-x}{2}} - e^{\frac{-2x}{\epsilon}}
\end{align}

Which is our uniform approximation for the whole solutions.

TODO: Compare vs exact solution and graph them.

TODO: Compute the second term?

\section{Total Quasi Steady State}

\subsection{A biological problem}

In enzyme kinetics we often come across chemical reactions lik

\begin{equation}
E + S \leftarrow k_1 \rightarrow k_{-1} C \rightarrow k_2 E + P
\end{equation}

This is an enzyme E and substrate S that combine, reversibly, to form a complex
C, which in turn gives us a product and enzyme back. $k_1$ etc are rate
constants. We won't bother with the biological etails too much, but instead look
at how we can analyze the above as a dynamical system.

The law of mass action tells us that two molecules a and b forming complex c,
$a+b \rightarrow k c$ are governed by $\frac{dc}{dt} = kab$. That is, the more
concentration of a or b we have, the faster they turn into complex c. This has
been experimentally verified in 1867 by Guldberg and Waage, and many times
others.

With this law, we can translate our chemical reaction a system of differential
equations.

\begin{align}
\frac{dE}{dt} &= -k_1(E+S) + k_{-1}C + k_2C, \\
\frac{dS}{dt} &= -k1(E+S) + k_{-1}C, \\
\frac{dC}{dt} &= k_1(E+S) - k_{-1}C - k_2C, \\
\frac{dP}{dt} &= k_2C
\end{align}

We also normally have some initial conditions to help us out: $S(0) = S_0, E(0)
= E_0, C(0)=0, P(0)=0$.

We see immediately that

\begin{equation}
\frac{dE}{dt} + \frac{dC}{dt} = 0
\end{equation}

with the initial conditions we get that

\begin{equation}
E + C = E_0
\end{equation}

Which we can use to eliminate E from the equation. This is a conservation law.
We also don't care about P, as it doesn't feed back into the other equations. We
are left with

\begin{align}
\frac{dS}{dt} &= -k_1(E_0 - C)S + k_{-1}C, \\
\frac{dC}{dt} &= k_1(E_0 - C)S - k_{-1}C - k_2 C, \\
\end{align}

This is a two-dimensional system, which is much easier to deal with. Can we do
better? It turns out we can.

Many times, the substrate has a much higher concentration than enzyme. This
means that C ban be treated as constant after a short initial period. By
expoliting this fact, we can reduce the system to one equation. This i called
the Michaelis-Menten or Quasi-Steady State approximation.

Assume that C is constant $\iff \frac{dC}{dt} = 0$ after a short period of time,
then we can could write (as $\frac{dC}{dt}$ could be treated as 0)

\begin{equation}
\frac{dS}{dt} = -k2 C
\end{equation}

with

\begin{equation}
C = \frac{E_0 S}{K_m +S}
\end{equation}

where $K_m = \frac{k_{-1} + k_2}{k_1}$ is the Michaelis-Menten constant.

\begin{equation}
\frac{dS}{dt} = \frac{-k_2 E_0 S}{K_M + S}
\end{equation}

together with $S(0) = S_0$ as initial condition is what is normally considered
the QSSA, valid after some short amount of time has passed.

We can justify this with a procedure simiilar to te previous section, as done by
for example Lin and Segel REF. Instead we are going to look at a slight
variation of this, namely what happens when the amount of substarate isn't much
bigger than the amount of enzymes. This is the Total Quasi Steady State
assumption, discovered by Borghans and Segel 1996.

\subsection{TQSSA}

The basic idea behind TQSSA is to perform a change of variable

\begin{equation}
\overline{S} = S + C
\end{equation}

TODO: This doesn't really make sense without talking about validity of QSSA.

This change of variable makes the approximation valid for more parameters. We
get the following system of equations from REF and REF

\begin{align}
\frac{d\overline{S}}{dt} &= - k_2 C, \\
\frac{dC}{dt} &= k_1(E_0-C)(\overline{S}-C)-(k_{-1}+ k_2) C
\end{align}

With initial conditions $\overline{S}(0)=S_0, C(0)=C$, these are the rate
equations for the TQSSA.

TODO: Show and explain phase plane for intuition.

TODO: Justify time scales chosen?

Borghans et al found these two following time scales,

\begin{align}
t_c &= \frac{1}{k_1(E_0+S_0+K_M)}, \\
t_{\overline{s}} &= \frac{E_0+S_0+K_m}{k_2+E_0}
\end{align}

$K_M = \frac{k_{-1}+k_2}{k_1}$ as before. We will now analyze this like before.
But first, we have to scale the system.

\subsection{Scaling}

We will now introduce scaled, dimensionless variables. We have the two times:

TODO: Include Segel def. of scaling (p11) (Slemrod ~441p)

\begin{align}
t_f &= \frac{t}{t_c}, \\
t_s &= \frac{t}{t_{\overline{S}}}
\end{align}

f for fast, s for slow/long. Also scaling C and $\overline{S}$ by their maximum:

\begin{align}
c &= \frac{C}{C_0}, \\
s &= \frac{\overline{S}}{S_0}
\end{align}

$S_0$ is max since it starts from something and then turns into complex. $C_0$
is max derived by Borghans et al. REF as

TODO: Show how Borghans derive this?

\begin{equation}
C_0 = \frac{E_0 S_0}{E_0 + S_0 + K_m}
\end{equation}

\subsection{Outer solution}

We now have a mathematical formulation of the problem and we just need a small
parameter $\epsilon$ to begin look for the outer solution, where the complex
changes slowly.

A necessary condition for TQSSA to hold is $0 \epsilon \leq 1$

TODO: Why don't Khoo use $\ll$? Odd.

\begin{equation}
\epsilon = \frac{t_f}{t_s}
\end{equation}

this comes from Borghans. \textit{Assuming} $t_f \ll t_s$ we then get, for the
outer region, using c and $t_s$

\end{document}

