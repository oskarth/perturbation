\documentclass[12pt]{article}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\linespread{1.5}

\begin{document}

\section{Introduction}

Perturbation theory has its roots in celestial mechanics and aerodynamics. There
are two particularly interesting victories the field has seen. The first was the
discovery of Neptune, and the second is the theoretical foundation for
aerodynamics. We will give a short account of these in the history section.

The promise of perturbation theory is that it allows us to solve a
larger class of problems than we could otherwise with analytical methods. It
does this by giving us approximate, but rigorous, solutions. In this article we
will guide the reader from some a very simple and familiar example, a quadratic
equation, all the way to a useful research example in system biology: the total
quasi steady state in enzyme kinetics.

The article has been written with the goal that any student with a basic
understanding of calculus, differential equations and linear algebra will be
able to follow along. All the relevant biology and perturbation theory will be
explained as we go along.

TODO: Write a subsection on the history of perturbation theory.

\section{A regular perturbation}

We are going to start with probably the simplest non-trivial example imaginable:
a quadratic equation.

\begin{equation}
x^2 - 2x + \epsilon = 0
\end{equation}

We know how to solve this analytically. The roots of the equation are $x = 1 +
\sqrt{1 - \epsilon}$ and $x = 1 - \sqrt{1 - \epsilon}$.

The case we are interested in is the one where $\epsilon$ is very small. In this
case, we can see that setting $\epsilon=0$ produces the roots $x=0$ and $x=2$
respectively. In fact, for any given small $\epsilon$ we notice that solution
changes very little. In that sense it's a very "boring" and predictable problem.

How can we make this observation more rigorous? One way is to rewrite the part
of the solution containing $\epsilon$ as a Taylor series. Recall that a Taylor
series for a function $f(x)$ around a looks like follows.

\begin{equation}
f(x) = \sum_{n=0}^{\infty} \frac{f^{n}(a)}{n!} (x-a)^n
\end{equation}

The Taylor series for $f(\epsilon) = (1 - \epsilon)^{1/2}$ around 0 is thus.

\begin{equation}
\sqrt{1 - \epsilon} = 1 - \frac{\epsilon}{2} - \frac{\epsilon^2}{8} + O(\epsilon^3)
\end{equation}

If we take the limit of this as $\epsilon \to 0$, it's obvious that our
intuition is correct. That is, a small change in epsilon only brings about a
small change in the solution.

\begin{align*}
x_1 &= \frac{\epsilon}{2} + \frac{\epsilon^2}{8} + O(\epsilon^3), \\
x_2 &= 2 - \frac{\epsilon}{2} - \frac{\epsilon^2}{8} + O(\epsilon^3)
\end{align*}

What is the point of all this? We said in the beginning that perturbation theory
allows us to solve a larger class of problems, but what we have done so far
hasn't given any indication of that being true. After all, we already have an
analytical formula for the quadratic equation.

Let's start over, but this time let's assume we don't have the quadratic formula
at our disposal. We will outline a method which would allow us to get arbitrarly
good approximations for polynomials of any degree.

Let's assume the solution of (1) can be expressed on the form of a power series
of $\epsilon$.

\begin{equation}
\sum_0^{\infty} a_k \epsilon^k = a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...
\end{equation}

We will now insert this power series into (1), and then expand that expression
in terms of powers of $\epsilon$. We only have to do this for the first few
terms of the power series, as we will see in the end. If we want to we can
always add more terms and get a more accurate approximation.

\begin{equation}
(a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...)^2 - 2(a_0 + a_1 \epsilon + a_2
\epsilon^2 + ...) + \epsilon = 0
\end{equation}

We expand the expression using Big-Oh algebra. For example, $(a_0 + a_1 \epsilon
+ a_2 \epsilon^2)^2 = a_0^2 + 2 a_0 a_1 \epsilon + (a_1^2 + 2 a_0 a_2) \epsilon^2 + O(\epsilon^3)$.

The whole expression becomes the following.

\begin{equation}
a_0^2 - 2 a_0 + (2 a_0 a_1 - 2 a_1 + 1)\epsilon + (a_1^2 + 2 a_0 a_2 - 2 a_2)
\epsilon^2 = O(\epsilon^3), \epsilon \to 0
\end{equation}

Now we turn the problem of determining the coefficients $a_0$, $a_1$ etc. Since
$\epsilon$ is a variable here rather than a parameter, we see that the
coefficients before each power of $\epsilon$ separately all have to be equal to
zero. For example, $a_0^2 - 2 a_0 = 0$ when $\epsilon \to 0$, and if we remove
that part and divide the rest by $\epsilon$ we get the same for the next
coefficient, etc. We thus get the following system of equations for solving the
coefficients.

\begin{align*}
a_0^2 - 2 a_0 &=0, \\
2 a_0 a_1 - 2 a_1 + 1 &= 0, \\
a_1^2 + 2 a_0 a_2 - 2 a_2 &= 0
\end{align*}

Solving these equations in turn gives us $a_0 = 0$ or $a_0 = 2$. For $a_0 = 0$ we
have $a_1 = \frac{1}{2}$ and $a_2 = \frac{1}{8}$. For $a_0 = 2$ we have $a_1 = -
\frac{1}{2}$ and $a_2 = - \frac{1}{8}$.

We said at the beginning that we assume the solution is on the form of a power
series of $\epsilon$. We have two options for the coefficients, and these
corresponds to the two approximate solutions.

\begin{align*}
x_1 &= \frac{1}{2} \epsilon + \frac{1}{8} \epsilon^2 + O(\epsilon^3), \\
x_2 &= 2 - \frac{1}{2} \epsilon - \frac{1}{8} \epsilon^2 + O(\epsilon^3)
\end{align*}

This corresponds well to our previous solution, without the use of the
quadratica formula. We have thus found a general method for finding approximate
solutions to polynomials of any degree with a small parameter $\epsilon$.

\section{A singular perturbation}

In the last section we dealt with a so called regular problem. In this section
we will deal with a singular perturbation problem. What is the difference? In a
singular perturbation problem, the small $\epsilon$ \textit{matters} for the
solution. In general, singular problems are generally interesting precisely
because their solutions can change a lot with just a small change in
circumstances.

As before, we will use a quadratic equation to illustrate how it works.

\begin{equation}
\epsilon x^2 - 2 x + 1 = 0
\end{equation}

As before, we take $\epsilon$ to be a very small number. This equation has the
following solutions.

\begin{align*}
x_1 &= \frac{1 + \sqrt{1 - \epsilon}}{\epsilon}, \\
x_2 &= \frac{1 - \sqrt{1 - \epsilon}}{\epsilon}.
\end{align*}

The first thing we notice is that even though $\epsilon$ is very small, we can't
set it to zero. If we were to do it in (8), we would only get a one degree
polynomial. The fact that we are losing solutions is a qualitative change, and
is indicative that we are dealing with a singular perturbation problem.

TODO: Skip this Taylor section, it's superfluous.

We can re-use the Taylor expansion of $\sqrt{1 - \epsilon}$ from (3), and we
would get that the solutions of our equation are.

TODO: Provide Taylor expansion.

\begin{equation}
x_1 = \frac{1 + \sqrt{1 - \epsilon}}{\epsilon}, \\
x_2 = \frac{1 - \sqrt{1 - \epsilon}}{\epsilon}.
\end{equation}

We have a problem here. As $\epsilon \to 0$, the root $x_2 \to \infty$. Is this
correct? Let's use the same method a we did before. We assume the solution can
be expressed in the form of a power series of $\epsilon$. Inserting this in our
equation gives us the following.

\begin{equation}
\epsilon (a_0 + a_1 \epsilon + a_2 \epsilon^2)^2 - 2(a_0 + \epsilon a_1 +
a_2 \epsilon^2 + ...) + 1 = 0
\end{equation}

which gets expanded into.

\begin{equation}
(- 2 a_0 + 1) + (a_0^2 - 2 a_1) \epsilon + (2 a_0 a_1 -2 a_2) \epsilon^2 +
O(\epsilon^3)
\end{equation}

and leads to.

\begin{align*}
- 2 a_0 + 1 &= 0, \\
a_0^2 - 2 a_1 &= 0, \\
2 a_0 a_1 -2 a_2 &= 0
\end{align*}

$a_0 = \frac{1}{2}$, $a_1=\frac{1}{8}$, $a_2= \frac{1}{64}$, but this gives us
only one of the roots, $x_1$.

\begin{equation}
x_1 = \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2)
\end{equation}

By the Fundamental Theorem of Algebra, we would expect to see two solutions.
What happened with the other root? We missed it because it's not on the form of
a perturbation series. We got a hint of this when we saw the Taylor expansion of
the root, but remember that we aren't supposed to use that. So what do we do?

The key here is that we can do a change in variable to turn the problem into a
regular perturbation problem.

\begin{equation}
x(\epsilon) = \frac{y(\epsilon)}{\delta(\epsilon)}
\end{equation}

Here we are treating $x$ as a function, $y(\epsilon)$ is $O(1)$ and we want to
determine the re-scaling factor function $\delta(\epsilon)$. Our original
equation becomes

\begin{equation}
\frac{\epsilon}{\delta^2} y^2 - \frac{2}{\delta} y + 1 = 0
\end{equation}

Our goal is to simplify the equation. We do this by dropping relatively
insignifcant terms. As we have seen, it turns out that the first term in the
equation is not insignificant, so we have to leave it in. Is there some other
term that we, to a first approximation, can drop?

All the three terms in the above equation have an order of magnitude. The method
of dominant balance tells us to look for pairs that balance, where balance means
they are of the same order of magnitude. We have already determined that the
first term can't be drop, so we have two options:

\textbf{Case 1.} $\frac{\epsilon}{\delta^2} y^2$ balances 1, with $\frac{2}{\delta}$
being relatively insignificant.

$\frac{\epsilon}{\delta^2} = 1$ implies $\delta = \epsilon^{\frac{1}{2}}$. But
then $\frac{2}{\delta} = \frac{2}{\epsilon^{\frac{1}{2}}}$ which isn't small as
$\epsilon \to 0$.

\textbf{Case 2.} $\frac{\epsilon}{\delta^2} y^2$ balance $\frac{2}{\delta} y$,
with 1 being relatively insignificant.

This means $\frac{\epsilon}{\delta^2} = \frac{1}{\delta}$ which implies that
$\delta = \epsilon$. This seems correct as both expressions are
$O(\frac{1}{e})$, and 1 is relatively small compared to that. We have

\begin{align}
P(x) = \epsilon x^2 - 2x + 1, \\
\epsilon P(\frac{y}{\epsilon}) = y^2 - 2 y + \epsilon
\end{align}

Where the last part is exactly the same as our regular perturbation problem.
Like before, this gives us

\begin{align}
y_1 &= \frac{1}{2} \epsilon + \frac{1}{8} \epsilon^2 + O(\epsilon^3), \\
y_2 &= 2 - \frac{1}{2} \epsilon - \frac{1}{8} \epsilon^2 + O(\epsilon^3)
\end{align}

which means that

\begin{align}
x_1 &= \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2), \\
x_2 &= \frac{2}{\epsilon} - \frac{1}{2} + \frac{1}{8} \epsilon + O(\epsilon^2)
\end{align}

The second root is our missing solution, and it corresponds to $x \to \infty$ as
$\epsilon \to 0$. This is the essence of singular perturbation theory - to find
the singular behavior and do a change of variable to turn it into a regular
perturbation problem.

\section{ODE and boundary theory}

We are now going to look at a differential equation.

\begin{equation}
\epsilon y'' + 2 y' + y, y(0)=0, y(1)=1, 0 < x < 1
\end{equation}

If we naively set $\epsilon = 0$ we see that the resulting equation $2 y' + y$
has the general solution $C e^{- \frac{1}{2}x}$. This can't satisfy both
boundary conditions at once. If it satisfies $y(0)=0$ we have $y=0$ as the only
solution, and if it satisfies $y(1)=1$ we have

\begin{equation}
y=e^{\frac{1}{2} (1 - x)}
\end{equation}

TODO: Why is this assumption legit?

We are going to assume this is a good approximation somewhere. We are also going
to assume this is valid for the $y(1)=1$ boundary condition.

Like the example in the last section, we missed something when we set $\epsilon
= 0$. We have found one of the two solutions and now we want to the find the
other one with the help of pairwise balancing. Since $\epsilon y''$ can't be
neglected, we have two options:



\section{Total Quasi Steady State}

\end{document}
