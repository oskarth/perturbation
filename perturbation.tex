\documentclass[12pt]{report}

\usepackage{hyperref}

\newcommand{\link}[2]{\href{#1}{#2}}

% \usepackage[utf8]{inputenc}
% \usepackage{mathtools}
\usepackage[parfill]{parskip} % line skip paragraphs
\linespread{1.5} % linespacing 1.5

\usepackage{amsmath} % good standards

\begin{document}

\bibliographystyle{plain}

\title{DRAFT: Singular Pertubation Theory and TQSSA}
\author{Oskar Thor\'{e}n}

\maketitle

\tableofcontents

\chapter{Introduction}

\textbf{TODO:} Write this section last. Example: Lin p. 277 about Prandtl and drag.

\textit{It causeth my head to ache} -- Newton on the orbits of the moon. Three
body problem. Discovery Neptunus, but also Vulcan (GR fixed that). Poincare and Prandtl.

Many phenomena can be modelled by ODEs, but only a subset of ODEs can be solved
exactly. We need approximate solutions that are justified in a rigorous manner.

This text is heavily inspired by two source. The first is
\textit{A first look at Perturbation Theory} by Simmonds. The other is
\textit{Applied mathematics in...} by Lin and Segel. Also taking some
inspiration from Holmes \textit{Introduction to Perturbation methods...}

\chapter{Algebraic regular perturbation}

\textbf{TODO:} Elaborate on Series method procedure by Lin and Segel (p227)

The Series method for perturbation goes as follows. We assume a dependent variable can be expanded as a power series of a small parameter. Then we have the following steps A-E... (put them inline).

Let's start with a simple quadratic equation.

\begin{equation}
  z^2 - 2z + \epsilon
\end{equation}

For small $\epsilon$, how does the solution change? The first thought
we might have is that we simply set $\epsilon$ to zero. If we do that
we get two solutions, $z_1 = 2$ and $z_2 = 0$. In this case, since we
know the exact solution to the full equations $z = 1 \pm
\sqrt{1-\epsilon}$, it's easy to see that changing $\epsilon$ a bit
only changes the solution a bit. The solutions to the equation will be
around $0$ and $2$.

When we say that $\epsilon$ is small, we mean $0 < \epsilon
\ll 1$. The symbol $\ll$ means that the limit of $\epsilon$ over unity
approaches $0$.

One way to do this more rigorously is to expand the $\sqrt{1-\epsilon}$
part as a Taylor series around $\epsilon = 0$. (In this case, we
could've used a binominal series expansion as well.)

\textbf{TODO:} Write intermediate steps.

\textbf{QUESTION:} Big Oh review?

$\sqrt{1 - \epsilon}$ = $1 - \frac 1 2 \epsilon - \frac 1 8 \epsilon^2
+ O(\epsilon^3)$

The Big-oh notation is just a way to sweep irrelevant arithmetic
details under the rug. What it's saying is that the following terms
won't grow faster than on the order of $\epsilon^2$
(which, as $\epsilon$ is close to zero, is essentially insignificant).

\textbf{TODO:} Show these steps.

Using that Taylor expansion in our original solution, and performing some basic
Big-Oh algebra, we get $z_1 = \frac 1 2 \epsilon + O(\epsilon^2)$ and $z_2 = 2 -
\frac 1 2 \epsilon + O(\epsilon^2)$. Here we are content with the $O(\epsilon^2)$
term at the end, as more details aren't necessary for our current purposes.

What does this tell us? What we have established is a way to get successively better
approximations to the solutions, given that $\epsilon$ is very small. The
reduced equation corresponds to setting $\epsilon$ to $0$. This is called the 0th order perturbation.
If we want the first order perturbation, we get the solutions as seen above. You can imagine
substituting $\epsilon$ for a very small number, and you have some idea for how
the solutions behaves.

Let's assume we don't know exact solution to this quadratic equation. Instead
we'll assume that we can express the solution using a perturbation series, that
is every root of $P_\epsilon(z) = z^2 - 2z + \epsilon$ has a perturbation
expansion for some N. A perturbation expansion is defined as follows:

\begin{equation}
  z(\epsilon) = a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...  + R^{n+1}, R^{n+1} = O(\epsilon^{N+1}
\end{equation}

How do we go about doing this? What we want to do is to determine the
coefficients $a_0$, $a_1$ etc. We do this by writing

$$P_\epsilon(z(\epsilon)) = P_\epsilon(a_0 + a_1\epsilon + ... +
O(\epsilon^{N+1})) = (a_0 + a_1\epsilon + ...)^2 - 2(a_0 + a1_\epsilon
+ ...) + \epsilon = 0$$

It's identically equal as $\epsilon \to 0$. Now we simply collect the
$\epsilon$ terms and do some Big-oh algebra to put it in the required
form for some N.

For example, if N=1 we have $z(\epsilon) = a_0 + a_1\epsilon +
O(\epsilon^2)$ and $z^2(\epsilon) = ... = a_0^2 + 2 a_0 a_1 \epsilon +
O(\epsilon^2)$.

\textbf{NOTE}: Skipping over a lot of steps here.

We finally get, for $N=2$, something like

$$P_\epsilon(z_\epsilon) = (a_0^2 - 2_{a_0}) + (2 a_0 a_1 - 2 a_1 +1)\epsilon + (a_1^2 + 2 a_0 a_2 -
2 a_2)\epsilon^2 + O(\epsilon^3) = 0$$

\textbf{NOTE:} this part was confusing with the identically=0 part. Include proof of theorem?

\textbf{NOTE:} make procedural nature step (a)-(e) (Lin and Segel) more clear

$(a_0^2 - 2_{a_0}) = 0$, since expression must be identically equal to zero as
$\epsilon \to 0$. If $P_\epsilon(z(\epsilon)) = 0$ for small $\epsilon$... (skipping this part).

Fundamental theorem of perturbation theory says that if we have such an
expansion, identically equal to 0, for $\epsilon$ sufficiently small, and all
the coefficients are independent of $\epsilon$ (i.e. factored out), then the
coefficients $A_0 = A_1 = ... = A_N = 0$. What this means for us that we get a
system of equations consisting of the following two equations:

$$ a_0^2 - 2_{a_0} = 0$$
$$ 2 a_0 a_1 - 2 a_1 +1 = 0 $$
$$ a_1^2 + 2 a_0 a_2 -2 a_2 = 0$$

We simple solve this like we normally would. $a_0 = 0$ or $a_0 = 2$. If
we pick the first solution, we get $a_1=\frac 1 2$, and that gives
$a_2 = \frac 1 8$.

This gives us the same perturbation series as when we knew the
solution of the quadratic equation, without actually knowing the exact
solution. That is, we could do the same on any polynomial of any
degree, giving us an approximate solution without knowing an exact
one.

\textbf{NOTE}: Write down definition of Big Oh

\chapter{Algebraic singular perturbation}

Let's look at a slightly more interesting example.

\begin{equation}
  \epsilon z^2 - 2z + 1
\end{equation}

In this example, assuming we are aware of the quadratic equation for
polynomials, we have $z = \frac{(1 \pm
  \sqrt{1-\epsilon})}{\epsilon}$. As $\epsilon \to 0$ we have a
singularity. If we don't know the exact solution, and simply try to
set $\epsilon=$, we note that we are removing one of the roots. This
is indicative of a singular perturbation problem - we are missing
something when we remove the error term.

If we proceed as we did in the last section, assuming we don't know
the exact solution, we get an expression of one of the roots, but we
don't get the other one. Why not? Because the exact root (if we Taylor
expand the exact solution, starts with $\frac 2 \epsilon$, which is
not on the form of a perturbation series) [This is a messy paragraph].

If we don't know the exact solution, we can still get that the missing
root $z_2(\epsilon)$ approaches $\infty$. We know that there must be
two roots, since the polynomial is of degree two. We got

$$Q_\epsilon(z) = \epsilon z^2 - 2z +1 = \epsilon[z - z_1(\epsilon)][z
  - z_2(\epsilon)] = \epsilon z^2 - \epsilon [z_1(\epsilon) +
  z_2(\epsilon)]z + \epsilon z_1(\epsilon)z_2(\epsilon)$$

\textbf{NOTE:} equality doesn't anove with $\epsilon$, what's up with
that?

If $z=0$, we have $1 = \epsilon z_1(\epsilon) z_2(\epsilon)$, so as
$\epsilon \to 0$, $z_2 \to \infty$ (inverse of $\epsilon$), since the
other root approaches a finite value.

We can make a simplification here: when $z_2(\epsilon)$ is large, $-2
z_2(\epsilon) + 1 \sim -2 z_2(\epsilon)$ ($f \sim g$ means f
asymptotic to g as $\epsilon$ approaches 0, $\frac f g \to 1$).

\textbf{NOTE: }things missing here

We then have $z_2 \sim \epsilon z_2^2 - 2 z_2$ leads to $z_2 \sim
\frac 2 \epsilon$ (sloppy writing). Just a heuristic, not rigorous
argument, but it leads to the conjecture:

$$\epsilon z_2(\epsilon) \sim b_0 \neq 0$$

$b_0 = 2$ doesn't matter, just that it's $\neq 0$. This suggest a
change of variable:

$$\epsilon z (\epsilon) = \omega (\epsilon)$$

A lot of notation missing. Essence is that we turn a singular
behavior, with the change of a variable, and reduce it into a regular
perturbation problem.

$$\epsilon Q(\frac \omega \epsilon) = \omega^2 - 2\omega + \epsilon$$

Now we get essentially the same as before with the regular problem.

$$\omega_2(\epsilon) = 2 - \frac 1 2 \epsilon + O(\epsilon^2)$$

which gives

$$z_2(\epsilon) = \frac 2 \epsilon - \frac 1 2 + O(\epsilon)$$

We have thus found an approximate expression of the other root, by
turning the singular problem into a regular one.

\chapter{Differential equation singular perturbation}

Let's take an ODE example with some boundary conditions. We can divide this into multiple steps: finding the outer solution, boundary, layer, matching, and composite expansion.

\textbf{NOTE:} this section will be re-written with procedure and
notation a la Holmes page 58-63 - more clear wrt boundary-layer
coordinates and scales, and less notation generally. Following is not
clear at all, especially towards the end. -- is it really? Only major difference I see is the matching limit, where Holmes seems to use a more simplistic/naive version of lim yo(0+)=yi(inf). The pairwise balancing is also different. I feel like Lin and Segel is the more grown up version, possibly. Holmes seem to confuse, me at least, the O(epsilon) and the length scale? O?(1/$\epsilon$)

\begin{equation}
  \epsilon \frac{d^2y}{dx^2} + 2 \frac{dy}{dx} + y = 0; 0 < x < 1, 0
  < \epsilon \ll 1,
\end{equation}

and boundary conditions $y(0)=0, y(1) =1$.

\section{Outer solution}

The naive reduced version, $\epsilon = 0$ gives $2 \frac{dy}{dx} + y = 0$, with general solution $y=K \exp(-\frac{1}{2} x)$

The problem here is that we can't satisfy both boundary
conditions. Either the approximation leads to $y=0$ or $y=exp(\frac 1
2 - \frac 1 2 x)$. So what does this mean? Is either approximation
good, and if so in which part of the interval?

Here Lin and Segel show the logic of this by solving it exactly. We
are gonna jump straight into the approximation part.

Let's assume that we can use that solution is an outer solution(the one
we have), that approximates the exact solution everywhere but a small layer
near a boundary.  Let's also assume that this boundary layer is near $x=0$ (as opposed to near $x=1$).

We require that the outer approximation $y_O$ satisfy the boundary
condition at $x=1$. Then we have

$$2 \frac{dy_O}{dx} + y_O = 0, y_O(1)=1$$

$$y_O(x) = exp[\frac 1 2 (1-x)]$$

\section{Boundary layer}

We let the boundary layer near $x=0$ have a thickness of order of
magnitude $\delta(\epsilon)$. This layer thickness approaches zero as
$\epsilon \to 0$, so $\delta \to 0$ as $\epsilon \to 0$.

We introduce a change of variable as a rescaling for the scale
appropriate for the boundary layer. Having a scale of order unity
wouldn't be appropriate here, which is why we introduce another scale
with the following change of variable (\textbf{NOTE}: This is what Holmes calls a boundary-layer coordinate, I think).

$$\xi = \frac x \delta$$

We now use this change of variable in our DE.

We use this function instead: $Y(\xi, \epsilon) = y(\xi \epsilon,
\epsilon)$ and get:

$$\frac \epsilon \delta^2 \frac{d^2Y}{d\xi^2} + \frac 2 \delta
\frac{dY}{d\xi} + Y = 0$$

\textbf{QUESTION:} Is pairwise balancing the same as method of dominant balance that Bender and Orszag describes?

We now do something called pairwise balancing, to determine the
relative magnitudes. $\delta$ is the length scale in the boundary
layer, and the various 'differentials' are O(1) there. The order of
magnitudes are

$$\frac \epsilon \delta^2, \frac 1 \delta, 1.$$

If two of these balance out, and the third is negligible, we can
neglect the last one and simplify the problem. We know that one term of
the balancing pair must be $\frac \epsilon \delta^2$, since we've seen
that an approximation valid everywhere in $(0,1]$ isn't obtainable
  without it.

If this first expression balances the third, then obviously $\delta =
\epsilon^{\frac 1 2}$, which implies $\frac 1 \delta$ is large and not
negliable. Alternatively, if the first expression balances the second,
$\delta = \epsilon$ and the third expression is negligable, and wehave
found a so called scaling, $\delta = \epsilon, \xi=\frac x \epsilon$.

\textbf{TODO:} Clarify above section with what it means to have a good balance.

All together we now have the inner equation, with a scale appropriate
at the boundary/inner layer.

$$\frac{d^2Y}{d\xi^2} + \frac{2dY}{d\xi} + \epsilon Y = 0$$

As a first approximation, like before, we neglect the term with
$\epsilon$ in it and get an approximate equation for the boundary
layer. As $\epsilon \to 0$, we get it in terms of $y_i$:

$$\frac{d^2y_I}{d\xi^2} + \frac{2dy_I}{d\xi} = 0$$

We apply the boundary condition $y_I(0) = 0$. The second one we
discard for now, since x=1 is not on the same scale as the one we are
operating on now, $\delta=\epsilon$.

$$y_I(\xi) = C(1-exp^{-2\xi})$$

\section{Matching}

Now we turn to the matter of matching these two approximations, each
valid at their own scale. This is done by overlaping the two solutions
at some edge or intermediate region, where both solutions are good
approximations.

According to Lin and Segel, this is an area of continuous study, but
the way we will do it is by noticing that when $x=O(\delta)$, x is
within boundary layer, and when $x=O(1)$ it's outside it. How to do we
x to be in the intermediate region? We let $x=O[\Theta(\epsilon)]$
where $O(\Theta)$ is between the two magnitudes

\begin{equation}
\lim \epsilon \to 0 \frac \Theta \delta = \infty, \lim \epsilon \to 0 \Theta = 0
\end{equation}

\textbf{TODO}:  Elaborate more! A lot of things missing.(Lin/Segel page 294).

We find a limit which has to hold for both of them, a matching
requirement. To get a uniform approximation valid for $[0,1]$, we add the
inner and outer solution and remove the common part.

Earlier with $x=O(\delta)$, in the boundary layer, we introduced a new
variable $\xi$ and let $\epsilon \to 0$ with $\xi$ fixed. We now do
something similar for the intermediate region.

$$n = \frac x \Theta (x = n \Theta, \xi = \frac{n \Theta}{\delta})$$

The inner and outer approximations match if they have a common
limit. Matching requires that

$$\lim \epsilon \to 0[y_O(x) |_{x=n \Theta}] = lim \epsilon \to
0[y_I(\xi)|_{xi=n \frac \Theta \delta}], n fixed$$

Left-hand side:

$$\lim \epsilon \to 0[\exp^{(\frac 1 2)(1-x)} |_{x=n \Theta}] = \lim
\epsilon \to 0[\exp^{(\frac 1 2)(1- n \Theta)}] = \exp^{\frac 1 2} $$
[why $n \Theta \to 0$?  cause n is fixed and other $\to 0$.]

Right-hand side:

$$\lim \epsilon \to 0[C(1-\exp^{-\frac{2n \Theta}{\delta}})] = C$$

So we have a requirement that $C = \exp^{\frac 1 2}$

We thus have a C for our inner equation,
$y_I(\xi) = e^{\frac 1 2}(1-e^{-2\xi})$

\section{Composite expansion}

Now that we have an inner approximation and an outer approximation, we
would like to put them together into one uniform approximation $y_U$,
valid everywhere in $[0,1]$. We add them together and remove common part.

$$y_U(x) = y_O(x) + y_I(\frac x \delta) - \lim \epsilon \to 0 y_O(n \Theta)$$

\textbf{TODO:} Explain why the common part is that limit.

Sanity check should show that it produces inner approximation for
inner region, rest negligible etc. It should reduce to the right
approximation depending on region.

In total, $$y_U(x) = e^{(\frac 1 2)(1-x)} + e^{\frac 1 2}(1 -
		e^{-2\frac x \epsilon}) - e^{\frac 1 2} = e^{\frac 1 2}(e^{\frac -x 2}
			- e^{\frac -2x \epsilon})$$

\textbf{TODO:} Confirm numerically / analytically / qualitatively.

\textbf{TODO}: Write limits properly.

\chapter{Total Quasi Steady State application}

We will proceed similarly to how we did in the previous section. We start off by stating the problem,
and then turn it from a biological form to a mathematical form, which we can
solve as before. We do this with the help of dimensional analysis and scaling.

\section{Biological problem}

The Quasi Steady State Assumption (QSSA) is a way of simplifying a system of two differential equations, under assumptions which are commonly satisfied in the biological problem that the differential equations corresponds to.

If we have a substrate S and enzyme E that reversibly combines into a complex C, which in turn irreversibly produces a product P and frees up the enzyme, we can characterize this by the following ?-diagram.

\begin{equation}
E + S \leftarrow \rightarrow C \rightarrow P + E
\end{equation}

By the law of mass action, we can translate this ?-diagram into a set of differential equations.

\begin{align*}
\frac{dS}{dt} &= -k_1(E_0-C)S + k_{-1}C, \\
\frac{dC}{dt} &= k_1[(E_0-C)S + k_mC],
\end{align*}

with $K_m = \frac{k_{-1}+k_2}{k_1}$. With the help of conservation laws we can get $E$ and $P$. Initial conditions are $(S, E, C, P) = (S_0, E_0, 0, 0)$.

\textbf{QUESTION:} Does $\ll$ really mean asymptotically much less than? Seems like it is sometimes used as a "just small" fashion, and sometimes even as just $<1$.

The normal criteria used in the QSSA is $ E_0 \ll S_0 $. That is, the amount of substrate is much bigger than the amount of enzyme, which leads to a lot of reactions happening. After a short initial period, the concentration of C will essentially not change. This is the key in the QSSA - it means that we can approximate $dC/dt=0$, and then change our system of two differential equations into a system of one equation (expressing S in terms of C), which we can get an exact expression of.

How does the QSSA differ from that Total Quasi Steady State Assumption (TQSSA)? There we use the \textit{total substrate concentration} $\overline{S}$ instead of $S$ \cite{Borghans}, where $\overline{S} = S + C$. W then get the following rate equations for TQSSA.

\begin{equation}
\frac{d\overline{S}}{dt} = -k_2C;
\frac{dC}{dt} = k_1[(E_0-C)S + k_mC]
\end{equation}

Initial conditions $\overline{S}(0)=S_0, C(0)=0$.

With the problem formulated, we now proceed to justify TQSSA with the help of singular perturbation analysis and Khoo et. al. \cite{Khoo}.

If we look in the phase plane we should see that there are two stages.

\textbf{NOTE}: Show graph of phase plane.

\textbf{QUESTION}: How to provide it? Mathematica?

\section{Scaling}

Segel and Slemrod (1989) enumerates several essential points for appropriate scaling. The goal is to get scaled and dimensionless variables, which by construction are of unit order of magnitude.

\textbf{QUESTION}: Include a section on basics of dimensionless and scaling? ~ 6.3 in Lin/Segel

First, let's think about time scales. Khoo and Hegland mentions the two timescales Borghans et al. finds.

\begin{align*}
t_c &= \frac{1}{k_1(E_0+S_0+K_M)}, \\
t_{\overline{s}} &= \frac{E_0+S_0+K_m}{k_2+E_0}
\end{align*}

where $K_m = \frac{k_{-1} + k_{2}}{k_1}$ is the Michaelis-Menten constant.

\textbf{NOTE}: We could derive this using Segel and Slemrod.

Khoo and Heglund scale the time periods by dividing $t$ with each time scale, and the concentrations by dividing them with their maxima.

\begin{align*}
\tau = \frac t t_c, \\
T = \frac t t_{\overline{S}}
\end{align*}

The first time period is for the fast initial time scale, and the second time scale for the slow one after that.

\begin{equation}
c = \frac{[C]}{C_0}; s = \frac{[\overline{S}]}{S_0}
\end{equation}

Borghans et al. calculate the maximum of $[C]$ as follows $C_0 = E_0S_0 / (E_0 + S_0 + K_m)$

\textbf{NOTE}: Should we derive this? Probably. ~1-2 pages and learn something about Pade approximations.

\section{Outer solution}

We can reformulate the necessary condition of validity for TQSSA, derived by Borghans et al., $t_c \leq t_{\overline{S}}$ as $0\epsilon$ HERE ATM

\section{Inner solution}

$0 < \epsilon \leq 1, \epsilon = \frac{t_{\epsilon}}{t_{s}}$

\section{Matching and uniform approximation}

\textbf{QUESTION: } Error term?

\textbf{QUESTION: } Numerical simulation?

\section{Placeholder for possible temporary stuff}
Temp stuff in all sections below.

\textbf{TODO: } Write intro for biology and relevance.

The enzyme-substrate-complex schema

\begin{equation}
E + S \leftrightarrow C \rightarrow E + P
\end{equation}

where $k_1$, $k_{-1}$ and $k_2$ are parameters belonging to the
respective arrows, can be described by the follow set of differential
equations.

\textbf{TODO:} Show derivation with conservation laws ~1 page.

\textbf{TODO:} Fill out rest of the ODEs.

\begin{equation}
\frac{dS}{dT} = ... \\
\frac{dC}{dT} = ...
\end{equation}

with initial conditions $(S,E,C,P) = (S_0, E_0, 0, 0)$

\textbf{TODO:} Fill out description of QSSA.

The Quasi Steady State Assumptions tells us that we can approximate
... assuming ...

TQSSA is an extension of the standard QSSA by the use of a change of
variable. When we have too much enzyme for the standard QSSA to be
valid, we can sometimes replace the free substrate concentration by
total substrate concentration, as follows.

\begin{equation}
\overline{S} = S+C
\end{equation}

\textbf{TODO:} Add reference and derivation of ODE.

This changes the previous ODEs in ... to the following system of
differential equations.

\begin{align*}
\frac{d\overline{S}}{dt} &= -k_2C, \\
\frac{dC}{dt} &= k_1[(E_0 - C)(\overline{S}-C)-K_m C],
\end{align*}

with the same initial conditions as before. Before we can analyze
this, we have to scale it correctly. If we assume... assume what?

\textbf{TODO:} Show phase plane.

\textbf{TODO:} Where does the criteria for its validity come from? Explain

This is the criteria for its applicability
\begin{equation}
\frac{k_2 E_0}{k_1(E_0+S_0+K_m)^2} \ll 1
\end{equation}


\textbf{NOTE:} rough outline of topics. Once ODE section above is
clearer, all components there except for scaling and translating the
chemical reactions.  Section based on Lin/Segel chapter 10
(primarily), YZ-notes on QSSA and Khoo TQSSA-justification

In the paper there are three main model assumptions being made: law of
mass action, Michaelis-menten mechanics, and Total Quasi Steady
State. We will now attempt to justify these with singular perturbation
theory [all? TQSSA only?].

Let's start with a short description of them all [unclear if this should be included].

\subsection{Law of mass action}

States that \textit{the rate of a chemical reaction is proportional to the
product of the concentrations of the reactants}. Depends on the continuum
hypothesis and the well-stirred assumption \cite{Ingalls}.

\begin{equation}
a+b \rightarrow c
\end{equation}

\begin{equation}
\dot{c} = k_1 ab
\end{equation}

\subsection{Michaelis-Menten Mechanics}

In an enzyme-catalyzed reaction $$ s^* + e^* \rightleftharpoons
c^* \rightarrow p*+e* $$

we often see that the enyzyme concentration is very small compared to
the substrate concentration, this in turn allow us to approximate $\dot{c}$ to
0, which leads to a simplified system of equations.

\bibliography{references}

\end{document}
