\documentclass[12pt]{report}

\usepackage{hyperref}

\newcommand{\link}[2]{\href{#1}{#2}}

% \usepackage[utf8]{inputenc}
% \usepackage{mathtools}
\usepackage[parfill]{parskip} % line skip paragraphs
\linespread{1.5} % linespacing 1.5

\begin{document}

\bibliographystyle{plain}

\title{Singular Pertubation Theory and TQSSA}
\author{Oskar Thor\'{e}n}

\maketitle

\chapter{Singular perturbation theory}

\section{History and introduction}

\textit{It causeth my head to ache} -- Newton on the orbits of the moon. Three
body problem. Discovery Neptunus, but also Vulcan. Poincare and Prandtl.

Many phenomena can be modelled by ODEs, but only a subset of ODEs can be solved
exactly. We need approximate solutions that are justified in a rigorous manner.

This text is heavily inspired by two source. The first is \textit{A first look
at Perturbation Theory} by Simmonds. The other is \textit{Applied mathematics
in...} by Lin and Segel.

\section{Simple regular perturbation}

Let's start with a simple quadratic equation.

\begin{equation}
  z^2 - 2z + \epsilon
\end{equation}


For small $\epsilon$, how does the solution change? The first thought we might
have is that we simply set $\epsilon$ to zero. If we do that we get two
solutions, $z_1 = 2$ and $z_2 = 0$. In this case, since we know the exact
solution to the full equationis $z = 1 \pm \sqrt{1-\epsilon}$, it's easy to see
that changing $\epsilon$ a bit only changes the solution a bit. The solutions to
the equation will be around $0$ and $2$.

When we say that $\epsilon$ is small, we mean something like $0 < \epsilon
\ll 1$. The symbol $\ll$ means that the limit of $\epsilon$ over unity
approaches $0$.

One way to do this more rigorously is to expand $\sqrt{1-\epsilon}$ part as a
Taylor series around $\epsilon = 0$. In this case, we could've used a binominal series expansion as
well.

$\sqrt{1 - \epsilon}$ = $1 - \frac 1 2 \epsilon - \frac 1 8 \epsilon^2 +
O(\epsilon^3)$

The Big-oh notation is just a way to sweep irrelevant arithmetic details under
the rug. What it's saying is that the following terms won't grow faster than on
the order of $\epsilon^2$ (which, as $\epsilon$ is close to zero, is essentially
insignificant).

Using that Taylor expansion in our original solution, and performing some basic
Big-Oh algebra, we get $z_1 = \frac 1 2 \epsilon + O(\epsilon^2)$ and $z_2 = 2 -
\frac 1 2 \epsilon + O(\epsilon^2)$. Here I'm content with the $O(\epsilon^2)$
term at the end, as more details aren't necessary for our current purposes.

What does this tell us? What've established is a way to get successively better
approximations to the solutions, given that $\epsilon$ is very small. The
reduced equation corresponds to setting $\epsilon$ to $0$. This is called
something like the 0th order perturbation. If we want the first order
perturbation, we get the solutions as seen above. You can imagine
substituting $\epsilon$ for a very small number, and you have some idea for how
the solutions behaves.

Let's assume we don't know exact solution to this quadratic equation. Instead
we'll assume that we can express the solution using a perturbation series, that
is every root of $P_\epsilon(z) = z^2 - 2z + \epsilon$ has a perturbation
expansion for some N. A perturbation expansion is defined as follows:

\begin{equation}
  z(\epsilon) = a_0 + a_1 \epsilon + a_2 \epsilon^2 + ...  + R^{n+1}, R^{n+1} = O(\epsilon^{N+1})
\end{equation}

How do we go about doing this? What we want to do is to determine the
coefficients $a_0$, $a_1$ etc. We do this by writing

$$P_\epsilon(z(\epsilon)) = P_\epsilon(a_0 + a_1\epsilon + ... +
O(\epsilon^{N+1})) = (a_0 + a_1\epsilon + ...)^2 - 2(a_0 + a1_\epsilon + ...) +
\epsilon = 0$$

It's identically equal as $\epsilon \to 0$. Now we simply
collect the $\epsilon$ terms and do some Big-oh algebra to put it in the
required form for some N.

For example, if N=1 we have $z(\epsilon) = a_0 + a_1\epsilon + O(\epsilon^2)$
and $z^2(\epsilon) = ... = a_0^2 + 2 a_0 a_1 \epsilon + O(\epsilon^2)$.

Skipping over a lot of steps here.

We finally get, for $N=2$, something like

$$P_\epsilon(z_\epsilon) = (a_0^2 - 2_{a_0}) + (2 a_0 a_1 - 2 a_1 +1)\epsilon + (a_1^2 + 2 a_0 a_2 -
2 a_2)\epsilon^2 + O(\epsilon^3) = 0$$

$(a_0^2 - 2_{a_0}) = 0$, since expression must be identically equal to zero as
$\epsilon \to 0$. If $P_\epsilon(z(\epsilon)) = 0$ for small $\epsilon$... (skipping this part).

Fundamental theorem of perturbation theory says that if we have such an
expansion, identically equal to 0, for $\epsilon$ sufficiently small, and all
the coefficients are independent of $\epsilon$ (i.e. factored out), then the
coefficients $A_0 = A_1 = ... = A_N = 0$. What this means for us that we get a
system of equations consisting of the following two equations:

$$ a_0^2 - 2_{a_0} = 0$$
$$ 2 a_0 a_1 - 2 a_1 +1 = 0 $$
$$ a_1^2 + 2 a_0 a_2 -2 a_2 = 0$$

We simple solve this like we normall would. $a_0 = 0$ or $a_0 = 2$. If we pick
the first solution, we get $a_1=\frac 1 2$, and that gives $a_2 = \frac 1 8$.

This gives us the same perturbation series as when we knew the solution of the
quadratic equation, without actually knowing the exact solution. That is, we
could do the same on any polynomial of any degree, giving us an approximate
solution without knowing an exact one.


- Write down definition of Big Oh

\section{Simple singular perturbation}

\begin{equation}
  \epsilon z^2 - 2z + 1
\end{equation}

In this example we have $z = \frac{(1 \pm \sqrt{1-\epsilon})}{\epsilon}$. Note
that $\epsilon=0$ removes one of the roots. This is indicative of a singular
perturbation problem - we are missing something when we remove the error term.

Unlike in the last section, $z(\epsilon) = \frac 2 \epsilon + ...$, obviously a
problem here.

Perturbation series $z(\epsilon) = a_0 + a_1 \epsilon + a_2 \epsilon^2 + ... +
R^{n+1}, R^{n+1} = O(\epsilon{N+1})$

Procedure for this:

1. Substitute
2. Expand
3. Collect
4. Substitute original IC/BC
5. ...

\section{ODE example}

ODE with BC. Can't fulfil both conditions at once. So two solutions, oe inner
$O(\epsilon)$ and one other $O(1)$. Then combine (match) these to get a uniform
solution.

Lin and Segel example:

$$\epsilon \frac{d^2 y}{d x^2} + 2 \frac{dy}{dx} = 0; y(0)=0, y(1)=1, 0 <
\epsilon \ll 1.$$

If we take the reduced problem, see that the solution loses a dimension, and
can't satisfy both BCs at once. Is it still a useful approximation somewhere?

Introduce inner and outer layer. Assume outer equation is approximated by
reduced solution, except for in a thin layer of width $\epsilon$, around the
boundary layer. For simplicity, let's also assume this outer approximation
$y_o$ should satisfy the boundary condition at $x=1$ (in this example we can
figure out the exact solution, so we already know the boundary layer is near
$x=0$).

We then have eqn 23-24 from Lin and Segel.

Rescaling.

Inner equation.

Matching/intermediate region/matching.

Uniform aproximation.

\chapter{TQSSA}

\section{Biological problem}

Thing and enzyme. Combines fast. QSSA two time scales. TQSSA similar.

\section{Scaling}

Scale dimensionless. Two periods. Basics of dimensionaless.

\section{Inner solution}

$0 < \epsilon \leq 1, \epsilon = \frac{t_{\epsilon}}{t_{s}}$

\section{Outer solution}

\section{Matching and uniform approximation}

\section{Applying it}

See Verdugo.

\bibliography{references}

\end{document}
